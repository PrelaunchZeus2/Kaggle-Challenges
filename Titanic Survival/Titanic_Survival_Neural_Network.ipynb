{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "data = [train, test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null Values\n",
    "for df in data:\n",
    "    mean_age = df['Age'].mean()\n",
    "    df['Age'] = df['Age'].fillna(mean_age)\n",
    "    df['Age'] = df['Age'].astype(int)\n",
    "    mode_embarked = df['Embarked'].mode()[0]\n",
    "    df['Embarked'] = df['Embarked'].fillna(mode_embarked)\n",
    "    df['Embarked'] = df['Embarked'].astype(str)\n",
    "\n",
    "#drop string columns\n",
    "for df in data:\n",
    "    df.drop(columns = ['Name', 'Ticket', 'Cabin'], inplace = True)\n",
    "\n",
    "# Feature Engineering\n",
    "for df in data:\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = 1\n",
    "    df.loc[df['FamilySize'] > 1, 'IsAlone'] = 0\n",
    "    df['Fare_per_person'] = df['Fare'] / df['FamilySize']\n",
    "\n",
    "for df in data:\n",
    "    print (df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'FamilySize']\n",
    "num_cols = ['Age', 'Fare']\n",
    "\n",
    "def preprocess(df, cat_cols, num_cols):\n",
    "    encoder = OneHotEncoder(sparse_output = False)\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    #encode columns\n",
    "    encoded_cols = encoder.fit_transform(df[cat_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(cat_cols))\n",
    "    df = pd.concat([df.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    #scale columns\n",
    "    scaled_cols = scaler.fit_transform(df[num_cols])\n",
    "    scaled_df = pd.DataFrame(scaled_cols, columns=num_cols)\n",
    "    df[num_cols] = scaled_df\n",
    "    \n",
    "    #drop old columns\n",
    "    df.drop(columns=cat_cols + num_cols, inplace=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "test = preprocess(test, cat_cols, num_cols)\n",
    "train = preprocess(train, cat_cols, num_cols)\n",
    "\n",
    "#manually add missing column because I'm not sure how to do it automatically\n",
    "train['Parch_9'] = 0\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.columns)\n",
    "print(train.columns)\n",
    "print(test.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#create train test split\n",
    "X = train.drop(columns = 'Survived')\n",
    "y = train['Survived']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "# Hypermodel\n",
    "class TitanicHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        \n",
    "        for i in range(hp.Int('num_layers', 1, 5)):\n",
    "            units = hp.Int('units_' + str(i), \n",
    "                           min_value=64, \n",
    "                           max_value=1024, \n",
    "                           step=32)\n",
    "            activation = hp.Choice('activation_' + str(i), ['relu', 'leaky_relu'])\n",
    "\n",
    "            kernel_regularizer = regularizers.l2(hp.Float('l2_regularization_' + str(i),\n",
    "                                                          min_value=1e-7,\n",
    "                                                          max_value=0.1,\n",
    "                                                          sampling='LOG'))\n",
    "            model.add(layers.Dense(units=units,\n",
    "                                   activation=activation,\n",
    "                                   kernel_regularizer=kernel_regularizer))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(hp.Float('dropout_' + str(i),\n",
    "                                              min_value=0.1,\n",
    "                                              max_value=0.5,\n",
    "                                              step=0.1)))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        optimizer = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
    "        if optimizer == 'adam':\n",
    "            optimizer = keras.optimizers.Adam(\n",
    "                learning_rate=hp.Float('learning_rate',\n",
    "                                       min_value=0.001,\n",
    "                                       max_value=0.01,\n",
    "                                       sampling='LOG')\n",
    "            )\n",
    "        else:\n",
    "            optimizer = keras.optimizers.RMSprop(\n",
    "                learning_rate=hp.Float('learning_rate',\n",
    "                                       min_value=0.0001,\n",
    "                                       max_value=0.01,\n",
    "                                       sampling='LOG')\n",
    "            )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def get_build_config(self):\n",
    "        return {}\n",
    "\n",
    "    def build_from_config(self, config):\n",
    "        return self.build(config)\n",
    "\n",
    "tuner = Hyperband(\n",
    "    TitanicHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_epochs=100,\n",
    "    factor=3,\n",
    "    directory='my_dir',\n",
    "    project_name='Titanic Survival Prediction'\n",
    ")\n",
    "\n",
    "#*********************#\n",
    "MY_PATIENCE = 10\n",
    "MY_EPOCHS = 100\n",
    "MY_MIN_DELTA = 1e-4\n",
    "#*********************#\n",
    "\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=MY_EPOCHS,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=MY_PATIENCE,\n",
    "        min_delta=MY_MIN_DELTA,\n",
    "        restore_best_weights=True\n",
    "    )],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(best_hyperparameters.values)\n",
    "print(best_model.summary())\n",
    "\n",
    "best_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=best_hyperparameters.get('learning_rate')\n",
    "    ),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "best_model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=MY_EPOCHS,  # Ensure you specify the number of epochs\n",
    "    callbacks=[keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=MY_PATIENCE,\n",
    "        min_delta=MY_MIN_DELTA,\n",
    "        restore_best_weights=True\n",
    "    )],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "epochs_range = range(1, len(history.history['accuracy']) + 1)\n",
    "sns.lineplot(x=epochs_range, y=history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "sns.lineplot(x=epochs_range, y=history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.title('Training and Validation Accuracy over Epochs', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss vs val_loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_test contains the true labels for the test set\n",
    "y_pred_prob = best_model.predict(X_valid)\n",
    "\n",
    "# Convert probabilities to class labels using a threshold (e.g., 0.5)\n",
    "y_pred_class = (y_pred_prob > 0.5).astype(\"int32\")\n",
    "\n",
    "# Compute ROC curve and ROC area\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Example: Adjust threshold to 0.6\n",
    "new_threshold = 0.6\n",
    "y_pred_class_adjusted = (y_pred_prob > new_threshold).astype(\"int32\")\n",
    "\n",
    "# Compute confusion matrix for adjusted threshold\n",
    "cm_adjusted = confusion_matrix(y_test, y_pred_class_adjusted)\n",
    "\n",
    "# Plot confusion matrix for adjusted threshold\n",
    "disp_adjusted = ConfusionMatrixDisplay(confusion_matrix=cm_adjusted, display_labels=[0, 1])\n",
    "disp_adjusted.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix (Adjusted Threshold)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict_classes(test)\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "#binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "predictions_df = pd.DataFrame({\n",
    "    'PassengerId': test['PassengerId'],\n",
    "    #'Survived': binary_predictions.flatten()\n",
    "    'Survived': predictions.flatten()\n",
    "})\n",
    "\n",
    "# Save predictions to CSV\n",
    "predictions_df.to_csv('Titanic_NN_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
