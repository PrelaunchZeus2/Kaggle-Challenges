{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "data = [train, test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass     Sex  Age  SibSp  Parch     Fare Embarked  \\\n",
      "0            1         0       3    male   22      1      0   7.2500        S   \n",
      "1            2         1       1  female   38      1      0  71.2833        C   \n",
      "2            3         1       3  female   26      0      0   7.9250        S   \n",
      "3            4         1       1  female   35      1      0  53.1000        S   \n",
      "4            5         0       3    male   35      0      0   8.0500        S   \n",
      "\n",
      "   FamilySize  IsAlone  Fare_per_person  \n",
      "0           2        0          3.62500  \n",
      "1           2        0         35.64165  \n",
      "2           1        1          7.92500  \n",
      "3           2        0         26.55000  \n",
      "4           1        1          8.05000  \n",
      "   PassengerId  Pclass     Sex  Age  SibSp  Parch     Fare Embarked  \\\n",
      "0          892       3    male   34      0      0   7.8292        Q   \n",
      "1          893       3  female   47      1      0   7.0000        S   \n",
      "2          894       2    male   62      0      0   9.6875        Q   \n",
      "3          895       3    male   27      0      0   8.6625        S   \n",
      "4          896       3  female   22      1      1  12.2875        S   \n",
      "\n",
      "   FamilySize  IsAlone  Fare_per_person  \n",
      "0           1        1         7.829200  \n",
      "1           2        0         3.500000  \n",
      "2           1        1         9.687500  \n",
      "3           1        1         8.662500  \n",
      "4           3        0         4.095833  \n"
     ]
    }
   ],
   "source": [
    "# Null Values\n",
    "for df in data:\n",
    "    mean_age = df['Age'].mean()\n",
    "    df['Age'] = df['Age'].fillna(mean_age)\n",
    "    df['Age'] = df['Age'].astype(int)\n",
    "    mode_embarked = df['Embarked'].mode()[0]\n",
    "    df['Embarked'] = df['Embarked'].fillna(mode_embarked)\n",
    "    df['Embarked'] = df['Embarked'].astype(str)\n",
    "\n",
    "#drop string columns\n",
    "for df in data:\n",
    "    df.drop(columns = ['Name', 'Ticket', 'Cabin'], inplace = True)\n",
    "\n",
    "# Feature Engineering\n",
    "for df in data:\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = 1\n",
    "    df.loc[df['FamilySize'] > 1, 'IsAlone'] = 0\n",
    "    df['Fare_per_person'] = df['Fare'] / df['FamilySize']\n",
    "\n",
    "for df in data:\n",
    "    print (df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'FamilySize']\n",
    "num_cols = ['Age', 'Fare']\n",
    "\n",
    "def preprocess(df, cat_cols, num_cols):\n",
    "    encoder = OneHotEncoder(sparse_output = False)\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    #encode columns\n",
    "    encoded_cols = encoder.fit_transform(df[cat_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(cat_cols))\n",
    "    df = pd.concat([df.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    #scale columns\n",
    "    scaled_cols = scaler.fit_transform(df[num_cols])\n",
    "    scaled_df = pd.DataFrame(scaled_cols, columns=num_cols)\n",
    "    df[num_cols] = scaled_df\n",
    "    \n",
    "    #drop old columns\n",
    "    df.drop(columns=cat_cols + num_cols, inplace=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "test = preprocess(test, cat_cols, num_cols)\n",
    "train = preprocess(train, cat_cols, num_cols)\n",
    "\n",
    "#manually add missing column because I'm not sure how to do it automatically\n",
    "train['Parch_9'] = 0\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'IsAlone', 'Fare_per_person', 'Pclass_1', 'Pclass_2',\n",
      "       'Pclass_3', 'Sex_female', 'Sex_male', 'SibSp_0', 'SibSp_1', 'SibSp_2',\n",
      "       'SibSp_3', 'SibSp_4', 'SibSp_5', 'SibSp_8', 'Parch_0', 'Parch_1',\n",
      "       'Parch_2', 'Parch_3', 'Parch_4', 'Parch_5', 'Parch_6', 'Parch_9',\n",
      "       'Embarked_C', 'Embarked_Q', 'Embarked_S', 'FamilySize_1',\n",
      "       'FamilySize_2', 'FamilySize_3', 'FamilySize_4', 'FamilySize_5',\n",
      "       'FamilySize_6', 'FamilySize_7', 'FamilySize_8', 'FamilySize_11'],\n",
      "      dtype='object')\n",
      "Index(['PassengerId', 'Survived', 'IsAlone', 'Fare_per_person', 'Pclass_1',\n",
      "       'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'SibSp_0', 'SibSp_1',\n",
      "       'SibSp_2', 'SibSp_3', 'SibSp_4', 'SibSp_5', 'SibSp_8', 'Parch_0',\n",
      "       'Parch_1', 'Parch_2', 'Parch_3', 'Parch_4', 'Parch_5', 'Parch_6',\n",
      "       'Embarked_C', 'Embarked_Q', 'Embarked_S', 'FamilySize_1',\n",
      "       'FamilySize_2', 'FamilySize_3', 'FamilySize_4', 'FamilySize_5',\n",
      "       'FamilySize_6', 'FamilySize_7', 'FamilySize_8', 'FamilySize_11',\n",
      "       'Parch_9'],\n",
      "      dtype='object')\n",
      "(418, 35)\n",
      "(891, 36)\n"
     ]
    }
   ],
   "source": [
    "print(test.columns)\n",
    "print(train.columns)\n",
    "print(test.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#create train test split\n",
    "X = train.drop(columns = 'Survived')\n",
    "y = train['Survived']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "# Hypermodel\n",
    "class TitanicHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        \n",
    "        for i in range(hp.Int('num_layers', 1, 5)):\n",
    "            units = hp.Int('units_' + str(i), \n",
    "                           min_value=64, \n",
    "                           max_value=1024, \n",
    "                           step=32)\n",
    "            activation = hp.Choice('activation_' + str(i), ['relu', 'leaky_relu'])\n",
    "\n",
    "            kernel_regularizer = regularizers.l2(hp.Float('l2_regularization_' + str(i),\n",
    "                                                          min_value=1e-7,\n",
    "                                                          max_value=0.1,\n",
    "                                                          sampling='LOG'))\n",
    "            model.add(layers.Dense(units=units,\n",
    "                                   activation=activation,\n",
    "                                   kernel_regularizer=kernel_regularizer))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(hp.Float('dropout_' + str(i),\n",
    "                                              min_value=0.1,\n",
    "                                              max_value=0.5,\n",
    "                                              step=0.1)))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                learning_rate=hp.Float('learning_rate',\n",
    "                                       min_value=0.001,\n",
    "                                       max_value=0.01,\n",
    "                                       sampling='LOG')\n",
    "            ),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "tuner = Hyperband(\n",
    "    TitanicHyperModel(),\n",
    "    objective='val_accuracy',  # Change this to 'val_loss'\n",
    "    max_epochs=100,\n",
    "    factor=3,\n",
    "    directory='my_dir',\n",
    "    project_name='Titanic Survival Prediction'\n",
    ")\n",
    "\n",
    "#*********************#\n",
    "MY_PATIENCE = 10\n",
    "MY_EPOCHS = 100\n",
    "MY_MIN_DELTA = 1e-4\n",
    "#*********************#\n",
    "\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=MY_EPOCHS,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=MY_PATIENCE,\n",
    "        min_delta=MY_MIN_DELTA,\n",
    "        restore_best_weights=True\n",
    "    )],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(best_hyperparameters.values)\n",
    "print(best_model.summary())\n",
    "\n",
    "best_model.compile(\n",
    "    optimizer=best_hyperparameters.get('optimizer'),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "best_model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid)\n",
    ")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "sns.lineplot(data=history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.title('Training and Validation Accuracy over Epochs', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss vs val_loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual vs predicted values\n",
    "y_preds = best_model.predict(X_valid)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_valid, y_preds, alpha=0.5)\n",
    "plt.plot([y_valid.min(), y_valid.max()], [y_valid.min(), y_valid.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted House Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(test)\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "predictions_df = pd.DataFrame({\n",
    "    'PassengerId': test['PassengerId'],\n",
    "    'Survived': binary_predictions.flatten()\n",
    "})\n",
    "\n",
    "# Save predictions to CSV\n",
    "predictions_df.to_csv('Titanic_NN_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
