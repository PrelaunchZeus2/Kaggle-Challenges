{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "data = [train, test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null Values\n",
    "for df in data:\n",
    "    mean_age = df['Age'].mean()\n",
    "    df['Age'] = df['Age'].fillna(mean_age)\n",
    "    df['Age'] = df['Age'].astype(int)\n",
    "    mode_embarked = df['Embarked'].mode()[0]\n",
    "    df['Embarked'] = df['Embarked'].fillna(mode_embarked)\n",
    "    df['Embarked'] = df['Embarked'].astype(str)\n",
    "\n",
    "#drop string columns\n",
    "for df in data:\n",
    "    df.drop(columns = ['Name', 'Ticket', 'Cabin'], inplace = True)\n",
    "\n",
    "# Feature Engineering\n",
    "for df in data:\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = 1\n",
    "    df.loc[df['FamilySize'] > 1, 'IsAlone'] = 0\n",
    "    df['Fare_per_person'] = df['Fare'] / df['FamilySize']\n",
    "\n",
    "for df in data:\n",
    "    print (df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'FamilySize']\n",
    "num_cols = ['Age', 'Fare']\n",
    "\n",
    "def preprocess(df, cat_cols, num_cols):\n",
    "    encoder = OneHotEncoder(sparse_output = False)\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    #encode columns\n",
    "    encoded_cols = encoder.fit_transform(df[cat_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(cat_cols))\n",
    "    df = pd.concat([df.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    #scale columns\n",
    "    scaled_cols = scaler.fit_transform(df[num_cols])\n",
    "    scaled_df = pd.DataFrame(scaled_cols, columns=num_cols)\n",
    "    df[num_cols] = scaled_df\n",
    "    \n",
    "    #drop old columns\n",
    "    df.drop(columns=cat_cols + num_cols, inplace=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "test = preprocess(test, cat_cols, num_cols)\n",
    "train = preprocess(train, cat_cols, num_cols)\n",
    "\n",
    "#manually add missing column because I'm not sure how to do it automatically\n",
    "train['Parch_9'] = 0\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.columns)\n",
    "print(train.columns)\n",
    "print(test.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#create train test split\n",
    "X = train.drop(columns = 'Survived')\n",
    "y = train['Survived']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 157 Complete [00h 00m 06s]\n",
      "val_loss: 0.6146777868270874\n",
      "\n",
      "Best val_loss So Far: 0.4133809804916382\n",
      "Total elapsed time: 00h 13m 48s\n",
      "\n",
      "Search: Running Trial #158\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "2                 |2                 |num_layers\n",
      "896               |416               |units_0\n",
      "leaky_relu        |leaky_relu        |activation_0\n",
      "0.00046307        |8.0308e-07        |l2_regularization_0\n",
      "0.4               |0.3               |dropout_0\n",
      "adam              |rmsprop           |optimizer\n",
      "0.0012583         |0.0038783         |learning_rate\n",
      "352               |288               |units_1\n",
      "relu              |leaky_relu        |activation_1\n",
      "2.294e-06         |4.2044e-07        |l2_regularization_1\n",
      "0.3               |0.5               |dropout_1\n",
      "704               |352               |units_2\n",
      "leaky_relu        |relu              |activation_2\n",
      "0.0067845         |0.0052908         |l2_regularization_2\n",
      "0.5               |0.2               |dropout_2\n",
      "960               |256               |units_3\n",
      "relu              |leaky_relu        |activation_3\n",
      "1.2558e-05        |4.0368e-05        |l2_regularization_3\n",
      "0.4               |0.1               |dropout_3\n",
      "384               |832               |units_4\n",
      "relu              |leaky_relu        |activation_4\n",
      "7.6431e-07        |1.0889e-07        |l2_regularization_4\n",
      "0.3               |0.3               |dropout_4\n",
      "4                 |100               |tuner/epochs\n",
      "0                 |34                |tuner/initial_epoch\n",
      "3                 |4                 |tuner/bracket\n",
      "0                 |4                 |tuner/round\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 78\u001b[0m\n\u001b[0;32m     75\u001b[0m MY_MIN_DELTA \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m#*********************#\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMY_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMY_PATIENCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMY_MIN_DELTA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     89\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m best_model \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_models(num_models\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     92\u001b[0m best_hyperparameters \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    242\u001b[0m     ):\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    255\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras_tuner\\src\\tuners\\hyperband.py:427\u001b[0m, in \u001b[0;36mHyperband.run_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m     fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtuner/epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    426\u001b[0m     fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtuner/initial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[1;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras_tuner\\src\\engine\\tuner.py:232\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"For AutoKeras to override.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03mDO NOT REMOVE this function. AutoKeras overrides the function to tune\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m    The fit history.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m--> 232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mfit(hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras_tuner\\src\\engine\\tuner.py:162\u001b[0m, in \u001b[0;36mTuner._try_build\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_build\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp):\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# clean-up TF graph from previously stored (defunct) graph\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     keras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[1;32m--> 162\u001b[0m     \u001b[43mgc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_hypermodel(hp)\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Stop if `build()` does not return a valid model.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "# Hypermodel\n",
    "class TitanicHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        \n",
    "        for i in range(hp.Int('num_layers', 1, 5)):\n",
    "            units = hp.Int('units_' + str(i), \n",
    "                           min_value=64, \n",
    "                           max_value=1024, \n",
    "                           step=32)\n",
    "            activation = hp.Choice('activation_' + str(i), ['relu', 'leaky_relu'])\n",
    "\n",
    "            kernel_regularizer = regularizers.l2(hp.Float('l2_regularization_' + str(i),\n",
    "                                                          min_value=1e-7,\n",
    "                                                          max_value=0.1,\n",
    "                                                          sampling='LOG'))\n",
    "            model.add(layers.Dense(units=units,\n",
    "                                   activation=activation,\n",
    "                                   kernel_regularizer=kernel_regularizer))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(hp.Float('dropout_' + str(i),\n",
    "                                              min_value=0.1,\n",
    "                                              max_value=0.5,\n",
    "                                              step=0.1)))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        optimizer = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
    "        if optimizer == 'adam':\n",
    "            optimizer = keras.optimizers.Adam(\n",
    "                learning_rate=hp.Float('learning_rate',\n",
    "                                       min_value=0.001,\n",
    "                                       max_value=0.01,\n",
    "                                       sampling='LOG')\n",
    "            )\n",
    "        else:\n",
    "            optimizer = keras.optimizers.RMSprop(\n",
    "                learning_rate=hp.Float('learning_rate',\n",
    "                                       min_value=0.0001,\n",
    "                                       max_value=0.01,\n",
    "                                       sampling='LOG')\n",
    "            )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def get_build_config(self):\n",
    "        return {}\n",
    "\n",
    "    def build_from_config(self, config):\n",
    "        return self.build(config)\n",
    "\n",
    "tuner = Hyperband(\n",
    "    TitanicHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_epochs=100,\n",
    "    factor=3,\n",
    "    directory='my_dir',\n",
    "    project_name='Titanic Survival Prediction'\n",
    ")\n",
    "\n",
    "#*********************#\n",
    "MY_PATIENCE = 10\n",
    "MY_EPOCHS = 100\n",
    "MY_MIN_DELTA = 1e-4\n",
    "#*********************#\n",
    "\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=MY_EPOCHS,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=MY_PATIENCE,\n",
    "        min_delta=MY_MIN_DELTA,\n",
    "        restore_best_weights=True\n",
    "    )],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(best_hyperparameters.values)\n",
    "print(best_model.summary())\n",
    "\n",
    "best_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=best_hyperparameters.get('learning_rate')\n",
    "    ),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "best_model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=MY_EPOCHS,  # Ensure you specify the number of epochs\n",
    "    callbacks=[keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=MY_PATIENCE,\n",
    "        min_delta=MY_MIN_DELTA,\n",
    "        restore_best_weights=True\n",
    "    )],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "epochs_range = range(1, len(history.history['accuracy']) + 1)\n",
    "sns.lineplot(x=epochs_range, y=history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "sns.lineplot(x=epochs_range, y=history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.title('Training and Validation Accuracy over Epochs', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss vs val_loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_test contains the true labels for the test set\n",
    "y_pred_prob = best_model.predict(X_valid)\n",
    "\n",
    "# Convert probabilities to class labels using a threshold (e.g., 0.5)\n",
    "y_pred_class = (y_pred_prob > 0.5).astype(\"int32\")\n",
    "\n",
    "# Compute ROC curve and ROC area\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Example: Adjust threshold to 0.6\n",
    "new_threshold = 0.6\n",
    "y_pred_class_adjusted = (y_pred_prob > new_threshold).astype(\"int32\")\n",
    "\n",
    "# Compute confusion matrix for adjusted threshold\n",
    "cm_adjusted = confusion_matrix(y_test, y_pred_class_adjusted)\n",
    "\n",
    "# Plot confusion matrix for adjusted threshold\n",
    "disp_adjusted = ConfusionMatrixDisplay(confusion_matrix=cm_adjusted, display_labels=[0, 1])\n",
    "disp_adjusted.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix (Adjusted Threshold)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict_classes(test)\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "#binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "predictions_df = pd.DataFrame({\n",
    "    'PassengerId': test['PassengerId'],\n",
    "    #'Survived': binary_predictions.flatten()\n",
    "    'Survived': predictions.flatten()\n",
    "})\n",
    "\n",
    "# Save predictions to CSV\n",
    "predictions_df.to_csv('Titanic_NN_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
